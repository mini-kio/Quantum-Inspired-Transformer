#!/usr/bin/env python3
"""
Quantum-Inspired Transformer í…ŒìŠ¤íŠ¸ ì½”ë“œ
Shape ì˜¤ë¥˜ ë° ê¸°ë³¸ ë™ì‘ í™•ì¸ì„ ìœ„í•œ ì¢…í•© í…ŒìŠ¤íŠ¸
"""

import torch
import torch.nn as nn
import numpy as np
import pytest
import sys
import os
import traceback
from typing import Dict, Any, Tuple

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from architecture.transformer import QuantumInspiredTransformer, QuantumInspiredTransformerEncoder
from architecture.attention import QuantumInspiredAttention
from architecture.position_encoding import PositionalEncoding, QuantumPositionalEncoding
from architecture.feed_forward import FeedForward, DualStateFeedForward
from architecture.integrated_layer import IntegratedTransformerLayer
from core.dual_state import DualStateRepresentation, DualStateController
from core.state_management import GlobalStateManager
from core.collapse import StateCollapseFramework
from optimization.resource_allocator import ResourceAllocator
from optimization.learning import UniversalLoss
from training.hyperparameters import HyperParameters


class QuantumTransformerTester:
    """Quantum-Inspired Transformer ì¢…í•© í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"í…ŒìŠ¤íŠ¸ ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì„¤ì •
        self.test_configs = {
            "tiny": {
                "d_model": 64,
                "nhead": 4,
                "num_layers": 2,
                "dim_feedforward": 128,
                "max_superposition_dim": 2,
                "vocab_size": 1000,
                "max_seq_len": 32
            },
            "small": {
                "d_model": 128,
                "nhead": 8,
                "num_layers": 4,
                "dim_feedforward": 256,
                "max_superposition_dim": 4,
                "vocab_size": 5000,
                "max_seq_len": 64
            },
            "base": {
                "d_model": 512,
                "nhead": 8,
                "num_layers": 6,
                "dim_feedforward": 2048,
                "max_superposition_dim": 4,
                "vocab_size": 10000,
                "max_seq_len": 128
            }
        }
        
        self.results = {}
    
    def test_dual_state_representation(self, config_name: str = "tiny"):
        """ì´ì¤‘ ìƒíƒœ í‘œí˜„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        print(f"\n=== ì´ì¤‘ ìƒíƒœ í‘œí˜„ í…ŒìŠ¤íŠ¸ ({config_name}) ===")
        config = self.test_configs[config_name]
        
        try:
            # ì´ì¤‘ ìƒíƒœ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            dual_state = DualStateRepresentation(
                hidden_dim=config["d_model"],
                max_superposition_dim=config["max_superposition_dim"]
            ).to(self.device)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            batch_size = 4
            seq_len = config["max_seq_len"]
            deterministic_state = torch.randn(
                batch_size, seq_len, config["d_model"]
            ).to(self.device)
            
            print(f"ì…ë ¥ í™•ì • ìƒíƒœ shape: {deterministic_state.shape}")
            
            # í™•ì • ìƒíƒœ â†’ ì¤‘ì²© ìƒíƒœ ë³€í™˜ í…ŒìŠ¤íŠ¸
            superposition_state = dual_state.to_superposition_state(deterministic_state)
            expected_superposition_shape = (
                batch_size, seq_len, 
                config["d_model"] * config["max_superposition_dim"]
            )
            print(f"ì¤‘ì²© ìƒíƒœ shape: {superposition_state.shape}")
            print(f"ì˜ˆìƒ shape: {expected_superposition_shape}")
            
            assert superposition_state.shape == expected_superposition_shape, \
                f"ì¤‘ì²© ìƒíƒœ shape ë¶ˆì¼ì¹˜: {superposition_state.shape} != {expected_superposition_shape}"
            
            # ì¤‘ì²© ìƒíƒœ â†’ í™•ì • ìƒíƒœ ë³€í™˜ í…ŒìŠ¤íŠ¸
            recovered_state = dual_state.from_superposition_state(superposition_state)
            print(f"ë³µì›ëœ í™•ì • ìƒíƒœ shape: {recovered_state.shape}")
            
            assert recovered_state.shape == deterministic_state.shape, \
                f"ë³µì›ëœ ìƒíƒœ shape ë¶ˆì¼ì¹˜: {recovered_state.shape} != {deterministic_state.shape}"
            
            print("âœ… ì´ì¤‘ ìƒíƒœ í‘œí˜„ í…ŒìŠ¤íŠ¸ í†µê³¼")
            return True
            
        except Exception as e:
            print(f"âŒ ì´ì¤‘ ìƒíƒœ í‘œí˜„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
    
    def test_quantum_attention(self, config_name: str = "tiny"):
        """ì–‘ì ì˜ê° ì–´í…ì…˜ í…ŒìŠ¤íŠ¸"""
        print(f"\n=== ì–‘ì ì˜ê° ì–´í…ì…˜ í…ŒìŠ¤íŠ¸ ({config_name}) ===")
        config = self.test_configs[config_name]
        
        try:
            # ì–‘ì ì–´í…ì…˜ ì´ˆê¸°í™”
            attention = QuantumInspiredAttention(
                d_model=config["d_model"],
                nhead=config["nhead"],
                max_superposition_dim=config["max_superposition_dim"],
                dropout=0.1
            ).to(self.device)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            batch_size = 4
            seq_len = config["max_seq_len"]
            
            # í™•ì • ìƒíƒœ í…ŒìŠ¤íŠ¸
            deterministic_input = torch.randn(
                batch_size, seq_len, config["d_model"]
            ).to(self.device)
            
            print(f"í™•ì • ìƒíƒœ ì…ë ¥ shape: {deterministic_input.shape}")
            
            deterministic_output = attention.forward_deterministic(deterministic_input)
            print(f"í™•ì • ìƒíƒœ ì¶œë ¥ shape: {deterministic_output.shape}")
            
            assert deterministic_output.shape == deterministic_input.shape, \
                f"í™•ì • ìƒíƒœ ì¶œë ¥ shape ë¶ˆì¼ì¹˜: {deterministic_output.shape} != {deterministic_input.shape}"
            
            # ì¤‘ì²© ìƒíƒœ í…ŒìŠ¤íŠ¸
            superposition_input = torch.randn(
                batch_size, seq_len, 
                config["d_model"] * config["max_superposition_dim"]
            ).to(self.device)
            
            print(f"ì¤‘ì²© ìƒíƒœ ì…ë ¥ shape: {superposition_input.shape}")
            
            superposition_output = attention.forward_superposition(superposition_input)
            print(f"ì¤‘ì²© ìƒíƒœ ì¶œë ¥ shape: {superposition_output.shape}")
            
            assert superposition_output.shape == superposition_input.shape, \
                f"ì¤‘ì²© ìƒíƒœ ì¶œë ¥ shape ë¶ˆì¼ì¹˜: {superposition_output.shape} != {superposition_input.shape}"
            
            print("âœ… ì–‘ì ì˜ê° ì–´í…ì…˜ í…ŒìŠ¤íŠ¸ í†µê³¼")
            return True
            
        except Exception as e:
            print(f"âŒ ì–‘ì ì˜ê° ì–´í…ì…˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
    
    def test_position_encoding(self, config_name: str = "tiny"):
        """ìœ„ì¹˜ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸"""
        print(f"\n=== ìœ„ì¹˜ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ ({config_name}) ===")
        config = self.test_configs[config_name]
        
        try:
            # í‘œì¤€ ìœ„ì¹˜ ì¸ì½”ë”©
            pos_encoding = PositionalEncoding(
                d_model=config["d_model"],
                dropout=0.1
            ).to(self.device)
            
            # ì–‘ì ìœ„ì¹˜ ì¸ì½”ë”©
            quantum_pos_encoding = QuantumPositionalEncoding(
                d_model=config["d_model"],
                max_superposition_dim=config["max_superposition_dim"],
                dropout=0.1
            ).to(self.device)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°
            batch_size = 4
            seq_len = config["max_seq_len"]
            
            # í‘œì¤€ ìœ„ì¹˜ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
            input_tensor = torch.randn(
                batch_size, seq_len, config["d_model"]
            ).to(self.device)
            
            print(f"ì…ë ¥ í…ì„œ shape: {input_tensor.shape}")
            
            pos_encoded = pos_encoding(input_tensor)
            print(f"ìœ„ì¹˜ ì¸ì½”ë”© ì¶œë ¥ shape: {pos_encoded.shape}")
            
            assert pos_encoded.shape == input_tensor.shape, \
                f"ìœ„ì¹˜ ì¸ì½”ë”© ì¶œë ¥ shape ë¶ˆì¼ì¹˜: {pos_encoded.shape} != {input_tensor.shape}"
            
            # ì–‘ì ìœ„ì¹˜ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
            quantum_pos_encoded = quantum_pos_encoding(input_tensor)
            print(f"ì–‘ì ìœ„ì¹˜ ì¸ì½”ë”© ì¶œë ¥ shape: {quantum_pos_encoded.shape}")
            
            expected_quantum_shape = (
                batch_size, seq_len,
                config["d_model"] * config["max_superposition_dim"]
            )
            assert quantum_pos_encoded.shape == expected_quantum_shape, \
                f"ì–‘ì ìœ„ì¹˜ ì¸ì½”ë”© ì¶œë ¥ shape ë¶ˆì¼ì¹˜: {quantum_pos_encoded.shape} != {expected_quantum_shape}"
            
            print("âœ… ìœ„ì¹˜ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ í†µê³¼")
            return True
            
        except Exception as e:
            print(f"âŒ ìœ„ì¹˜ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
    
    def test_feed_forward(self, config_name: str = "tiny"):
        """í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸"""
        print(f"\n=== í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ ({config_name}) ===")
        config = self.test_configs[config_name]
        
        try:
            # í‘œì¤€ í”¼ë“œí¬ì›Œë“œ
            ff = FeedForward(
                d_model=config["d_model"],
                dim_feedforward=config["dim_feedforward"],
                dropout=0.1
            ).to(self.device)
            
            # ì´ì¤‘ ìƒíƒœ í”¼ë“œí¬ì›Œë“œ
            dual_ff = DualStateFeedForward(
                d_model=config["d_model"],
                dim_feedforward=config["dim_feedforward"],
                max_superposition_dim=config["max_superposition_dim"],
                dropout=0.1
            ).to(self.device)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°
            batch_size = 4
            seq_len = config["max_seq_len"]
            
            # í‘œì¤€ í”¼ë“œí¬ì›Œë“œ í…ŒìŠ¤íŠ¸
            input_tensor = torch.randn(
                batch_size, seq_len, config["d_model"]
            ).to(self.device)
            
            print(f"ì…ë ¥ í…ì„œ shape: {input_tensor.shape}")
            
            ff_output = ff(input_tensor)
            print(f"í”¼ë“œí¬ì›Œë“œ ì¶œë ¥ shape: {ff_output.shape}")
            
            assert ff_output.shape == input_tensor.shape, \
                f"í”¼ë“œí¬ì›Œë“œ ì¶œë ¥ shape ë¶ˆì¼ì¹˜: {ff_output.shape} != {input_tensor.shape}"
            
            # ì´ì¤‘ ìƒíƒœ í”¼ë“œí¬ì›Œë“œ í…ŒìŠ¤íŠ¸
            dual_output = dual_ff(input_tensor)
            print(f"ì´ì¤‘ ìƒíƒœ í”¼ë“œí¬ì›Œë“œ ì¶œë ¥ shape: {dual_output.shape}")
            
            # ì´ì¤‘ ìƒíƒœ ì¶œë ¥ì€ ì›ë³¸ í¬ê¸°ì™€ ê°™ì•„ì•¼ í•¨ (ë‚´ë¶€ì ìœ¼ë¡œ ë³€í™˜ í›„ ë‹¤ì‹œ ì›ë˜ í¬ê¸°ë¡œ)
            assert dual_output.shape == input_tensor.shape, \
                f"ì´ì¤‘ ìƒíƒœ í”¼ë“œí¬ì›Œë“œ ì¶œë ¥ shape ë¶ˆì¼ì¹˜: {dual_output.shape} != {input_tensor.shape}"
            
            print("âœ… í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ í†µê³¼")
            return True
            
        except Exception as e:
            print(f"âŒ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
    
    def test_integrated_layer(self, config_name: str = "tiny"):
        """í†µí•© íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ í…ŒìŠ¤íŠ¸"""
        print(f"\n=== í†µí•© íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ í…ŒìŠ¤íŠ¸ ({config_name}) ===")
        config = self.test_configs[config_name]
        
        try:
            # í†µí•© ë ˆì´ì–´ ì´ˆê¸°í™”
            layer = IntegratedTransformerLayer(
                d_model=config["d_model"],
                nhead=config["nhead"],
                dim_feedforward=config["dim_feedforward"],
                dropout=0.1,
                max_superposition_dim=config["max_superposition_dim"],
                layer_id=0,
                num_layers=config["num_layers"]
            ).to(self.device)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°
            batch_size = 4
            seq_len = config["max_seq_len"]
            input_tensor = torch.randn(
                batch_size, seq_len, config["d_model"]
            ).to(self.device)
            
            print(f"ì…ë ¥ í…ì„œ shape: {input_tensor.shape}")
            
            # ë ˆì´ì–´ í†µê³¼
            output = layer(input_tensor)
            print(f"ë ˆì´ì–´ ì¶œë ¥ shape: {output.shape}")
            
            assert output.shape == input_tensor.shape, \
                f"ë ˆì´ì–´ ì¶œë ¥ shape ë¶ˆì¼ì¹˜: {output.shape} != {input_tensor.shape}"
            
            print("âœ… í†µí•© íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ í…ŒìŠ¤íŠ¸ í†µê³¼")
            return True
            
        except Exception as e:
            print(f"âŒ í†µí•© íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
    
    def test_transformer_encoder(self, config_name: str = "tiny"):
        """íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” í…ŒìŠ¤íŠ¸"""
        print(f"\n=== íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” í…ŒìŠ¤íŠ¸ ({config_name}) ===")
        config = self.test_configs[config_name]
        
        try:
            # ì¸ì½”ë” ì´ˆê¸°í™”
            encoder = QuantumInspiredTransformerEncoder(
                d_model=config["d_model"],
                nhead=config["nhead"],
                num_layers=config["num_layers"],
                dim_feedforward=config["dim_feedforward"],
                dropout=0.1,
                max_superposition_dim=config["max_superposition_dim"]
            ).to(self.device)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°
            batch_size = 4
            seq_len = config["max_seq_len"]
            input_tensor = torch.randn(
                batch_size, seq_len, config["d_model"]
            ).to(self.device)
            
            print(f"ì…ë ¥ í…ì„œ shape: {input_tensor.shape}")
            
            # ì¸ì½”ë” í†µê³¼
            output = encoder(input_tensor)
            print(f"ì¸ì½”ë” ì¶œë ¥ shape: {output.shape}")
            
            assert output.shape == input_tensor.shape, \
                f"ì¸ì½”ë” ì¶œë ¥ shape ë¶ˆì¼ì¹˜: {output.shape} != {input_tensor.shape}"
            
            print("âœ… íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” í…ŒìŠ¤íŠ¸ í†µê³¼")
            return True
            
        except Exception as e:
            print(f"âŒ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
    
    def test_full_transformer(self, config_name: str = "tiny"):
        """ì „ì²´ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print(f"\n=== ì „ì²´ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ({config_name}) ===")
        config = self.test_configs[config_name]
        
        try:
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
            hyperparams = HyperParameters(
                d_model=config["d_model"],
                nhead=config["nhead"],
                num_layers=config["num_layers"],
                dim_feedforward=config["dim_feedforward"],
                vocab_size=config["vocab_size"],
                max_seq_len=config["max_seq_len"],
                max_superposition_dim=config["max_superposition_dim"],
                dropout=0.1,
                learning_rate=1e-4
            )
            
            # ì „ì²´ ëª¨ë¸ ì´ˆê¸°í™”
            model = QuantumInspiredTransformer(hyperparams).to(self.device)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° (í† í° ID)
            batch_size = 4
            seq_len = config["max_seq_len"]
            input_ids = torch.randint(
                0, config["vocab_size"], 
                (batch_size, seq_len)
            ).to(self.device)
            
            print(f"ì…ë ¥ í† í° shape: {input_ids.shape}")
            
            # ëª¨ë¸ ìˆœì „íŒŒ
            with torch.no_grad():
                output = model(input_ids)
                print(f"ëª¨ë¸ ì¶œë ¥ shape: {output.shape}")
                
                expected_output_shape = (batch_size, seq_len, config["vocab_size"])
                assert output.shape == expected_output_shape, \
                    f"ëª¨ë¸ ì¶œë ¥ shape ë¶ˆì¼ì¹˜: {output.shape} != {expected_output_shape}"
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° í…ŒìŠ¤íŠ¸
            model.train()
            output = model(input_ids)
            loss = output.sum()  # ê°„ë‹¨í•œ ì†ì‹¤ í•¨ìˆ˜
            loss.backward()
            
            # íŒŒë¼ë¯¸í„°ì— ê·¸ë˜ë””ì–¸íŠ¸ê°€ ê³„ì‚°ë˜ì—ˆëŠ”ì§€ í™•ì¸
            has_grad = any(p.grad is not None for p in model.parameters() if p.requires_grad)
            assert has_grad, "ê·¸ë˜ë””ì–¸íŠ¸ê°€ ê³„ì‚°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
            
            print("âœ… ì „ì²´ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ í†µê³¼")
            return True
            
        except Exception as e:
            print(f"âŒ ì „ì²´ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
    
    def test_memory_usage(self, config_name: str = "small"):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        print(f"\n=== ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ({config_name}) ===")
        config = self.test_configs[config_name]
        
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.reset_peak_memory_stats()
                
                initial_memory = torch.cuda.memory_allocated()
                print(f"ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory / 1024**2:.2f} MB")
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
            hyperparams = HyperParameters(
                d_model=config["d_model"],
                nhead=config["nhead"],
                num_layers=config["num_layers"],
                dim_feedforward=config["dim_feedforward"],
                vocab_size=config["vocab_size"],
                max_seq_len=config["max_seq_len"],
                max_superposition_dim=config["max_superposition_dim"],
                dropout=0.1,
                learning_rate=1e-4
            )
            
            # ëª¨ë¸ ì´ˆê¸°í™”
            model = QuantumInspiredTransformer(hyperparams).to(self.device)
            
            if torch.cuda.is_available():
                model_memory = torch.cuda.memory_allocated() - initial_memory
                print(f"ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {model_memory / 1024**2:.2f} MB")
            
            # ì¶”ë¡  í…ŒìŠ¤íŠ¸
            model.eval()
            batch_size = 8
            seq_len = config["max_seq_len"]
            input_ids = torch.randint(
                0, config["vocab_size"], 
                (batch_size, seq_len)
            ).to(self.device)
            
            with torch.no_grad():
                output = model(input_ids)
            
            if torch.cuda.is_available():
                peak_memory = torch.cuda.max_memory_allocated()
                print(f"ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {peak_memory / 1024**2:.2f} MB")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                torch.cuda.empty_cache()
            
            print("âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ í†µê³¼")
            return True
            
        except Exception as e:
            print(f"âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
    
    def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("=" * 60)
        print("Quantum-Inspired Transformer ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        test_methods = [
            "test_dual_state_representation",
            "test_quantum_attention", 
            "test_position_encoding",
            "test_feed_forward",
            "test_integrated_layer",
            "test_transformer_encoder",
            "test_full_transformer",
            "test_memory_usage"
        ]
        
        configs_to_test = ["tiny", "small"]
        passed_tests = 0
        total_tests = 0
        
        for config_name in configs_to_test:
            print(f"\n{'='*20} {config_name.upper()} ì„¤ì • í…ŒìŠ¤íŠ¸ {'='*20}")
            
            for test_method in test_methods:
                total_tests += 1
                try:
                    method = getattr(self, test_method)
                    if method(config_name):
                        passed_tests += 1
                        self.results[f"{test_method}_{config_name}"] = "PASS"
                    else:
                        self.results[f"{test_method}_{config_name}"] = "FAIL"
                except Exception as e:
                    print(f"âŒ {test_method} ({config_name}) ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    self.results[f"{test_method}_{config_name}"] = "ERROR"
        
        # ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 60)
        print("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        print(f"ì´ í…ŒìŠ¤íŠ¸: {total_tests}")
        print(f"í†µê³¼: {passed_tests}")
        print(f"ì‹¤íŒ¨: {total_tests - passed_tests}")
        print(f"ì„±ê³µë¥ : {passed_tests/total_tests*100:.1f}%")
        
        print("\nìƒì„¸ ê²°ê³¼:")
        for test_name, result in self.results.items():
            status = "âœ…" if result == "PASS" else "âŒ"
            print(f"{status} {test_name}: {result}")
        
        return passed_tests == total_tests


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = QuantumTransformerTester()
    
    try:
        success = tester.run_all_tests()
        
        if success:
            print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ í†µê³¼í–ˆìŠµë‹ˆë‹¤!")
            return 0
        else:
            print("\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            return 1
            
    except Exception as e:
        print(f"\nğŸ’¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        return 2


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
